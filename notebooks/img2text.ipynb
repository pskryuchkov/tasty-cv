{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2text\n",
    "# https://grok.com/chat/1c87d70c-502b-447f-a53f-b13833c55a61\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Параметры\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "IMG_SIZE = 224  # Уменьшенное разрешение для экономии памяти\n",
    "MAX_TEXT_LENGTH = 50  # Максимальная длина текста в токенах\n",
    "VOCAB_SIZE = 5000  # Размер словаря\n",
    "LEARNING_RATE = 1e-5\n",
    "VALIDATION_SPLIT = 0.1  # 10% данных для валидации\n",
    "\n",
    "\n",
    "# 1. Кастомный датасет\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, data, img_dir, processor, tokenizer):\n",
    "        self.data = data\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.data.iloc[idx][\"img_path\"])\n",
    "        text = self.data.iloc[idx][\"text\"]\n",
    "\n",
    "        # Загрузка изображения\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Обработка изображения и текста\n",
    "        encoding = self.processor(\n",
    "            images=img,\n",
    "            text=text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_TEXT_LENGTH,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "\n",
    "# 2. Создание кастомного словаря\n",
    "def build_custom_vocab(texts, vocab_size):\n",
    "    # Токенизация текстов (простая, на основе слов)\n",
    "    nltk.download(\"punkt\")\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "        all_words.extend(words)\n",
    "\n",
    "    # Ограничение словаря\n",
    "    word_counts = Counter(all_words)\n",
    "    vocab = {\n",
    "        word: idx + 4\n",
    "        for idx, (word, _) in enumerate(word_counts.most_common(vocab_size - 4))\n",
    "    }\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    vocab[\"<START>\"] = 1\n",
    "    vocab[\"<END>\"] = 2\n",
    "    vocab[\"<UNK>\"] = 3\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# 3. Загрузка и подготовка данных\n",
    "def prepare_data(csv_file, img_dir):\n",
    "    # Чтение CSV\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Разделение на train и validation\n",
    "    train_data, val_data = train_test_split(\n",
    "        data, test_size=VALIDATION_SPLIT, random_state=42\n",
    "    )\n",
    "\n",
    "    # Создание кастомного словаря\n",
    "    vocab = build_custom_vocab(data[\"text\"].values, VOCAB_SIZE)\n",
    "\n",
    "    # Загрузка процессора и токенизатора BLIP\n",
    "    processor = BlipProcessor.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\", image_size=IMG_SIZE\n",
    "    )\n",
    "    tokenizer = processor.tokenizer\n",
    "\n",
    "    # Датасеты\n",
    "    train_dataset = ImageTextDataset(train_data, img_dir, processor, tokenizer)\n",
    "    val_dataset = ImageTextDataset(val_data, img_dir, processor, tokenizer)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, vocab, processor\n",
    "\n",
    "\n",
    "# 4. Дообучение модели\n",
    "def train_model(model, train_loader, val_loader, optimizer, scaler, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Тренировка\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():  # Mixed precision\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "                with autocast():\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "        model.train()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 5. Инференс (пример генерации текста)\n",
    "def generate_caption(model, processor, image_path, max_length=MAX_TEXT_LENGTH):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pixel_values=inputs.pixel_values,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "# Основной блок\n",
    "def main():\n",
    "    # Путь к данным\n",
    "    csv_file = \"dataset.csv\"\n",
    "    img_dir = \"images/\"\n",
    "\n",
    "    # Подготовка данных\n",
    "    train_loader, val_loader, vocab, processor = prepare_data(csv_file, img_dir)\n",
    "\n",
    "    # Загрузка модели\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-base\"\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Заморозка ViT для экономии памяти\n",
    "    for param in model.vision_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Оптимизатор и mixed precision\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Дообучение\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    model = train_model(model, train_loader, val_loader, optimizer, scaler, EPOCHS)\n",
    "\n",
    "    # Сохранение модели\n",
    "    model.save_pretrained(\"blip_finetuned\")\n",
    "    processor.save_pretrained(\"blip_finetuned\")\n",
    "\n",
    "    # Пример инференса\n",
    "    test_image = os.path.join(img_dir, train_loader.dataset.data.iloc[0][\"img_path\"])\n",
    "    caption = generate_caption(model, processor, test_image)\n",
    "    print(f\"Generated caption for {test_image}: {caption}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
