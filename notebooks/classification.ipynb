{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "# https://grok.com/chat/94d35e69-f6b6-4402-9107-14616814fa5e\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Параметры\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "num_classes = 3  # Укажите количество классов в вашем датасете\n",
    "data_dir = \"dataset\"  # Путь к датасету\n",
    "model_name = \"google/vit-base-patch16-224\"  # Предобученная модель ViT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Подготовка данных\n",
    "# Трансформации для предобработки изображений\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # ViT ожидает изображения 224x224\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=feature_extractor.image_mean, std=feature_extractor.image_std\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=feature_extractor.image_mean, std=feature_extractor.image_std\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Загрузка датасета\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"train\"), transform=train_transforms\n",
    ")\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"val\"), transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "# 2. Загрузка модели\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes,  # Количество классов\n",
    "    ignore_mismatched_sizes=True,  # Игнорировать несоответствие размеров классификатора\n",
    ")\n",
    "\n",
    "# Перенос модели на устройство\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Настройка оптимизатора и функции потерь\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 4. Функция для обучения\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Обучение\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # Сохранение лучшей модели\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_vit_model.pth\")\n",
    "            print(\"Saved best model with Val Accuracy: {:.2f}%\".format(best_acc))\n",
    "\n",
    "\n",
    "# 5. Запуск обучения\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# 6. Сохранение финальной модели\n",
    "torch.save(model.state_dict(), \"final_vit_model.pth\")\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
